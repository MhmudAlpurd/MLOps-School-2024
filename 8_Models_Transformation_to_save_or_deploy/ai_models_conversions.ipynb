{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-31 17:13:51.500851: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-10-31 17:13:51.516194: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-10-31 17:13:51.520632: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-10-31 17:13:51.530678: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-10-31 17:13:52.311386: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf \n",
    "import numpy as np "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "(train_images, train_labels), (test_images, test_labels) = tf.keras.datasets.mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images  = train_images.reshape(-1, 28, 28, 1).astype(\"float32\")/255.0\n",
    "test_images = test_images.reshape(-1, 28, 28, 1).astype('float32')/255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/amin/anaconda3/lib/python3.11/site-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1730382265.949969   54993 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-10-31 17:14:25.994657: W tensorflow/core/common_runtime/gpu/gpu_device.cc:2343] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n"
     ]
    }
   ],
   "source": [
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1)), \n",
    "    tf.keras.layers.MaxPooling2D((2, 2)), \n",
    "    tf.keras.layers.Conv2D(64, (3, 3), activation= 'relu'), \n",
    "    tf.keras.layers.MaxPooling2D((2, 2)), \n",
    "    tf.keras.layers.Flatten(), \n",
    "    tf.keras.layers.Dense(64, activation='relu'), \n",
    "    tf.keras.layers.Dense(10, activation='softmax')\n",
    "\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "\u001b[1m1688/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 8ms/step - accuracy: 0.8937 - loss: 0.3484 - val_accuracy: 0.9845 - val_loss: 0.0541\n",
      "Epoch 2/2\n",
      "\u001b[1m1688/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 7ms/step - accuracy: 0.9828 - loss: 0.0538 - val_accuracy: 0.9832 - val_loss: 0.0586\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x75143b9ef7d0>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "model.fit(train_images, train_labels, epochs=2, validation_split=0.1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#keras\n",
    "model.save('mnist_saved_keras.keras')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    }
   ],
   "source": [
    "#h5\n",
    "model.save('mnist_saved_h5.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/tmpbh5en9qv/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/tmpbh5en9qv/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved artifact at '/tmp/tmpbh5en9qv'. The following endpoints are available:\n",
      "\n",
      "* Endpoint 'serve'\n",
      "  args_0 (POSITIONAL_ONLY): TensorSpec(shape=(None, 28, 28, 1), dtype=tf.float32, name='keras_tensor')\n",
      "Output Type:\n",
      "  TensorSpec(shape=(None, 10), dtype=tf.float32, name=None)\n",
      "Captures:\n",
      "  128727551080400: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  128727551079824: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  128727551081360: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  128727551082128: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  128727551080976: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  128727551081936: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  128727551080208: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  128727551080784: TensorSpec(shape=(), dtype=tf.resource, name=None)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "W0000 00:00:1730382612.237566   54993 tf_tfl_flatbuffer_helpers.cc:392] Ignored output_format.\n",
      "W0000 00:00:1730382612.237600   54993 tf_tfl_flatbuffer_helpers.cc:395] Ignored drop_control_dependency.\n",
      "2024-10-31 17:20:12.238257: I tensorflow/cc/saved_model/reader.cc:83] Reading SavedModel from: /tmp/tmpbh5en9qv\n",
      "2024-10-31 17:20:12.239037: I tensorflow/cc/saved_model/reader.cc:52] Reading meta graph with tags { serve }\n",
      "2024-10-31 17:20:12.239065: I tensorflow/cc/saved_model/reader.cc:147] Reading SavedModel debug info (if present) from: /tmp/tmpbh5en9qv\n",
      "2024-10-31 17:20:12.242797: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:388] MLIR V1 optimization pass is not enabled\n",
      "2024-10-31 17:20:12.243653: I tensorflow/cc/saved_model/loader.cc:236] Restoring SavedModel bundle.\n",
      "2024-10-31 17:20:12.268203: I tensorflow/cc/saved_model/loader.cc:220] Running initialization op on SavedModel bundle at path: /tmp/tmpbh5en9qv\n",
      "2024-10-31 17:20:12.276048: I tensorflow/cc/saved_model/loader.cc:462] SavedModel load for tags { serve }; Status: success: OK. Took 37798 microseconds.\n",
      "2024-10-31 17:20:12.283145: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:268] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n"
     ]
    }
   ],
   "source": [
    "#tflite\n",
    "#Converter : Quantization (changing weights types : float32-->float16) + Prunning (removing not important connections) \n",
    "\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "tflite_model = converter.convert()\n",
    "with open('mnist_saved_tflite.tflite', \"wb\") as f :\n",
    "    f.write(tflite_model)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1730382824.145069   54993 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "I0000 00:00:1730382824.146099   54993 devices.cc:67] Number of eligible GPUs (core count >= 8, compute capability >= 0.0): 1\n",
      "2024-10-31 17:23:44.146171: I tensorflow/core/grappler/clusters/single_machine.cc:361] Starting new session\n",
      "I0000 00:00:1730382824.146391   54993 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-10-31 17:23:44.147479: W tensorflow/core/common_runtime/gpu/gpu_device.cc:2343] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n",
      "I0000 00:00:1730382824.311432   54993 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "I0000 00:00:1730382824.312496   54993 devices.cc:67] Number of eligible GPUs (core count >= 8, compute capability >= 0.0): 1\n",
      "2024-10-31 17:23:44.312569: I tensorflow/core/grappler/clusters/single_machine.cc:361] Starting new session\n",
      "I0000 00:00:1730382824.312787   54993 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-10-31 17:23:44.313689: W tensorflow/core/common_runtime/gpu/gpu_device.cc:2343] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n"
     ]
    }
   ],
   "source": [
    "#ONNX\n",
    "import tf2onnx\n",
    "import onnx\n",
    "\n",
    "model.output_names = ['output']\n",
    "\n",
    "\n",
    "spec =(tf.TensorSpec((None, 28, 28, 1), tf.float32, name='input'), )\n",
    "onnx_model, _ = tf2onnx.convert.from_keras(model, input_signature=spec)\n",
    "onnx.save_model(onnx_model, 'mnist_saved_onnx.onnx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#size of output models\n",
    "import os\n",
    "\n",
    "\n",
    "def get_file_size(file_name, file_path):\n",
    "    file_size = os.path.getsize(file_path)\n",
    "    file_size = file_size / (1024 * 1024)\n",
    "    print(f\"{file_name} size: {file_size:.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "h5 format size: 1.43 MB\n",
      "keras format size: 1.43 MB\n",
      "onnx format size: 0.47 MB\n",
      "tflite format size: 0.47 MB\n"
     ]
    }
   ],
   "source": [
    "get_file_size('h5 format', 'mnist_saved_h5.h5')\n",
    "get_file_size('keras format', 'mnist_saved_keras.keras')\n",
    "get_file_size('onnx format', 'mnist_saved_onnx.onnx')\n",
    "get_file_size('tflite format', 'mnist_saved_tflite.tflite')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step\n",
      "predictied_class:  3\n",
      "h5_inference_time 0.08497786521911621\n",
      "FPS (frames per second):  11.767770317852664\n"
     ]
    }
   ],
   "source": [
    "#inference_time : h5\n",
    "import time\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "model = tf.keras.models.load_model('mnist_saved_h5.h5')\n",
    "#model.summary()\n",
    "\n",
    "\n",
    "def preprocess_image(image_path):\n",
    "    img = Image.open(image_path).convert('L')\n",
    "    img = img.resize((28, 28))\n",
    "\n",
    "    img_array = np.array(img)\n",
    "    img_array = img_array.astype('float32') / 255.0\n",
    "\n",
    "    img_array = img_array.reshape(1, 28, 28, 1)\n",
    "\n",
    "    return img_array\n",
    "\n",
    "image_path = 'three.jpg'\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "test_image = preprocess_image(image_path)\n",
    "\n",
    "predictions = model.predict(test_image)\n",
    "predicted_class = np.argmax(predictions, axis=1)\n",
    "\n",
    "end_time = time.time()\n",
    "\n",
    "h5_inference_time = end_time - start_time\n",
    "\n",
    "print( 'predictied_class: ', predicted_class[0])\n",
    "print(\"h5_inference_time\", h5_inference_time)\n",
    "\n",
    "print('FPS (frames per second): ', 1/h5_inference_time ) #---> almost realtime.(realtime > 10 FPS)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predictied_class:  3\n",
      "tflite_inference_time 0.00020360946655273438\n",
      "FPS (frames per second):  4911.36299765808\n"
     ]
    }
   ],
   "source": [
    "#tflite inference time\n",
    "\n",
    "tflite_model_path = 'mnist_saved_tflite.tflite'\n",
    "\n",
    "\n",
    "interpreter = tf.lite.Interpreter(model_path=tflite_model_path)\n",
    "interpreter.allocate_tensors()\n",
    "\n",
    "\n",
    "input_details = interpreter.get_input_details()\n",
    "output_details = interpreter.get_output_details()\n",
    "\n",
    "\n",
    "image_path = 'three.jpg'\n",
    "\n",
    "\n",
    "test_image = preprocess_image(image_path)\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "interpreter.set_tensor(input_details[0]['index'], test_image)\n",
    "\n",
    "interpreter.invoke()\n",
    "\n",
    "predictions = interpreter.get_tensor(output_details[0]['index'])\n",
    "\n",
    "end_time = time.time()\n",
    "\n",
    "predicted_class = np.argmax(predictions, axis=1)\n",
    "tflite_inference_time = end_time - start_time\n",
    "\n",
    "print( 'predictied_class: ', predicted_class[0])\n",
    "print(\"tflite_inference_time\", tflite_inference_time)\n",
    "print('FPS (frames per second): ', 1/tflite_inference_time )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predictied_class:  3\n",
      "onnx_inference_time 0.0010268688201904297\n",
      "FPS (frames per second):  973.8342233573253\n"
     ]
    }
   ],
   "source": [
    "import onnx\n",
    "import onnxruntime as ort\n",
    "\n",
    "onnx_model_path = 'mnist_saved_onnx.onnx'\n",
    "onnx_model = onnx.load(onnx_model_path)\n",
    "\n",
    "\n",
    "#input_shape = onnx_model.graph.input[0].type.tensor_type.shape.dim\n",
    "#print(input_shape)\n",
    "\n",
    "\n",
    "session = ort.InferenceSession(onnx_model_path)\n",
    "\n",
    "input_name = session.get_inputs()[0].name\n",
    "output_name = session.get_outputs()[0].name\n",
    "\n",
    "\n",
    "img = 'three.jpg'\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "test_image = preprocess_image(img)\n",
    "\n",
    "predictions = session.run([output_name], {input_name : test_image})\n",
    "\n",
    "end_time = time.time()\n",
    "predicted_class = np.argmax(predictions[0], axis=1)\n",
    "\n",
    "onnx_inference_time = end_time - start_time\n",
    "\n",
    "print( 'predictied_class: ', predicted_class[0])\n",
    "print(\"onnx_inference_time\", onnx_inference_time)\n",
    "print('FPS (frames per second): ', 1/onnx_inference_time )\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "h5_inference_time 0.08497786521911621 seconds\n",
      "onnx_inference_time 0.0010268688201904297 seconds\n",
      "tflite_inference_time 0.00020360946655273438 seconds\n"
     ]
    }
   ],
   "source": [
    "print(\"h5_inference_time\", h5_inference_time, 'seconds')\n",
    "print(\"onnx_inference_time\", onnx_inference_time, 'seconds')\n",
    "print(\"tflite_inference_time\", tflite_inference_time , 'seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step\n",
      "h5_accuracy:  0.984\n",
      "tflite_accuracy:  0.984\n",
      "onnx_accuracy:  0.984\n"
     ]
    }
   ],
   "source": [
    "#Accuracy  --> onnx , h5 and tflite on mnist test set. \n",
    "\n",
    "import numpy as np\n",
    "import onnx \n",
    "import onnxruntime as ort\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras.models import load_model\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "x_test = x_test.astype('float32')/255.0\n",
    "\n",
    "#print(x_test.shape)\n",
    "\n",
    "\n",
    "x_test = np.expand_dims(x_test, axis=-1)\n",
    "#print(x_test.shape)\n",
    "\n",
    "\n",
    "#evaluating h5\n",
    "def evaluate_h5(h5_model_path, images):\n",
    "    model = load_model(h5_model_path)\n",
    "    y_pred = model.predict(images)\n",
    "    return np.argmax(y_pred, axis=-1)\n",
    "\n",
    "h5_model_path = 'mnist_saved_h5.h5'\n",
    "h5_predictions = evaluate_h5(h5_model_path, x_test)\n",
    "h5_accuracy = np.mean(h5_predictions==y_test)\n",
    "print('h5_accuracy: ', h5_accuracy)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#evaluating tflite\n",
    "def evaluate_tflite(tflite_model_path):\n",
    "    interpreter = tf.lite.Interpreter(model_path=tflite_model_path)\n",
    "    interpreter.allocate_tensors()\n",
    "    input_details = interpreter.get_input_details()\n",
    "    output_details = interpreter.get_output_details()\n",
    "\n",
    "\n",
    "    y_pred = []\n",
    "\n",
    "    for i in range(len(x_test)):\n",
    "        interpreter.set_tensor(input_details[0]['index'], x_test[i:i+1])\n",
    "        interpreter.invoke()\n",
    "        predictions = interpreter.get_tensor(output_details[0]['index'])\n",
    "        y_pred.append(np.argmax(predictions))\n",
    "\n",
    "\n",
    "    return np.array(y_pred)\n",
    "\n",
    "\n",
    "\n",
    "tflite_model_path = 'mnist_saved_tflite.tflite'\n",
    "tflite_predictions = evaluate_tflite(tflite_model_path)\n",
    "tflite_accuracy = np.mean(tflite_predictions==y_test)\n",
    "\n",
    "print('tflite_accuracy: ', tflite_accuracy)\n",
    "\n",
    "\n",
    "#evaluating onnx\n",
    "def evaluate_onnx(onnx_model_path):\n",
    "    session = ort.InferenceSession(onnx_model_path)\n",
    "    input_name = session.get_inputs()[0].name\n",
    "    output_name = session.get_outputs()[0].name\n",
    "\n",
    "    y_pred = []\n",
    "\n",
    "    for i in range(len(x_test)):\n",
    "        input_data = x_test[i:i+1].astype(np.float32)\n",
    "        predictions = session.run([output_name], {input_name : input_data})\n",
    "        y_pred.append(np.argmax(predictions[0], axis=-1))\n",
    "    return np.array(y_pred).flatten()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "onnx_model_path = 'mnist_saved_onnx.onnx'\n",
    "onnx_predictions = evaluate_onnx(onnx_model_path)\n",
    "onnx_accuracy = np.mean(onnx_predictions==y_test)\n",
    "\n",
    "print('onnx_accuracy: ', onnx_accuracy)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn \n",
    "import torch.optim as optim \n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.datasets import MNIST\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "\n",
    "class  SimpleCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleCNN, self).__init__()\n",
    "        self.conv1 =  nn.Conv2d(1, 32, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
    "        self.fc1 = nn.Linear(64*7*7, 128)\n",
    "        self.fc2 = nn.Linear(128, 10)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = nn.functional.relu(x)\n",
    "        x = nn.MaxPool2d(kernel_size=2)(x)\n",
    "        x = self.conv2(x)\n",
    "        x = nn.functional.relu(x)\n",
    "        x = nn.MaxPool2d(kernel_size=2)(x)\n",
    "        x = x.view(x.size(0), -1) #flatten\n",
    "        x = self.fc1(x)\n",
    "        x = nn.functional.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        return x \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 , Loss: 0.017918920144438744\n",
      "Epoch: 2 , Loss: 0.02787238359451294\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu' )\n",
    "\n",
    "#preprocessing, transformation\n",
    "transform  = transforms.Compose(\n",
    "    [\n",
    "        transforms.ToTensor(), \n",
    "        transforms.Normalize((0.5, ), (0.5, )) \n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "train_dataset = MNIST(root='./data', train=True, download=True, transform=transform)\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=64, shuffle=True)\n",
    "\n",
    "model = SimpleCNN().to(device)\n",
    "criterian = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "\n",
    "num_epochs = 2\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    for images, labels in train_loader:\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterian(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    print(f'Epoch: {epoch+1} , Loss: {loss.item()}' )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4 µs, sys: 1 µs, total: 5 µs\n",
      "Wall time: 9.54 µs\n"
     ]
    }
   ],
   "source": [
    "#saving pt, pth, onnx\n",
    "\n",
    "#pt\n",
    "torch.save(model, 'mnist_model_pytorch.pt') #architecture, weights\n",
    "\n",
    "\n",
    "#pth\n",
    "torch.save(model.state_dict(), 'mnist_model_pytorch.pth') #weights\n",
    "\n",
    "#onnx\n",
    "onnx_input = torch.randn(1, 1, 28, 28).to(device)\n",
    "torch.onnx.export(model, onnx_input, 'mnist_model_pytorch.onnx', input_names=['input'], output_names=['output'])\n",
    "\n",
    "%time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mnist_pt_size :  size: 1.61 MB\n",
      "mnist_pth_size :  size: 1.61 MB\n",
      "mnist_onnx_size :  size: 1.61 MB\n",
      "h5 format size: 1.43 MB\n",
      "keras format size: 1.43 MB\n",
      "onnx format size: 0.47 MB\n",
      "tflite format size: 0.47 MB\n"
     ]
    }
   ],
   "source": [
    "get_file_size('mnist_pt_size : ', 'mnist_model_pytorch.pt')\n",
    "get_file_size('mnist_pth_size : ', 'mnist_model_pytorch.pth')\n",
    "get_file_size('mnist_onnx_size : ', 'mnist_model_pytorch.onnx')\n",
    "get_file_size('h5 format', 'mnist_saved_h5.h5')\n",
    "get_file_size('keras format', 'mnist_saved_keras.keras')\n",
    "get_file_size('onnx format', 'mnist_saved_onnx.onnx')\n",
    "get_file_size('tflite format', 'mnist_saved_tflite.tflite')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pt predicted class:  3\n",
      "pt inference time:  0.0020055770874023438\n",
      "CPU times: user 4 µs, sys: 1 µs, total: 5 µs\n",
      "Wall time: 8.82 µs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_54993/2595358701.py:5: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model = torch.load('mnist_model_pytorch.pt')\n"
     ]
    }
   ],
   "source": [
    "#pt inferece\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "model = torch.load('mnist_model_pytorch.pt')\n",
    "model.to(device)\n",
    "\n",
    "model.eval()\n",
    "\n",
    "def load_and_preprocess(image_path):\n",
    "    image = Image.open(image_path).convert('L')\n",
    "    preprocess = transforms.Compose([\n",
    "        transforms.Resize((28, 28)), \n",
    "        transforms.ToTensor(), \n",
    "        transforms.Normalize((0.5, ), (0.5, ))\n",
    "    ])\n",
    "    image_tensor = preprocess(image)\n",
    "    return image_tensor.unsqueeze(0)\n",
    "\n",
    "\n",
    "image_path = 'three.jpg'\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "input_image  = load_and_preprocess(image_path).to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    output = model(input_image)\n",
    "    predicted_class = torch.argmax(output, dim=1).item()\n",
    "\n",
    "end_time = time.time()\n",
    "pt_inference_time = end_time - start_time\n",
    "\n",
    "print('pt predicted class: ', predicted_class)\n",
    "print('pt inference time: ', pt_inference_time)\n",
    "\n",
    "%time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pth inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#onnx inferecne"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#engine"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
